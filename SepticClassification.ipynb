{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026738ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/user/news/lib/python3.8/site-packages (2.6.0)\n",
      "Requirement already satisfied: nltk in /home/user/news/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: click in /home/user/news/lib/python3.8/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /home/user/news/lib/python3.8/site-packages (from nltk) (4.62.1)\n",
      "Requirement already satisfied: joblib in /home/user/news/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /home/user/news/lib/python3.8/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tensorflow in /home/user/news/lib/python3.8/site-packages (2.6.0)\n",
      "Requirement already satisfied: clang~=5.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: keras~=2.6 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.39.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: gast==0.4.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (44.0.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.34.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.4.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/news/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/news/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/user/news/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/user/news/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/user/news/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/user/news/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/user/news/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/user/news/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/user/news/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/user/news/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: seaborn in /home/user/news/lib/python3.8/site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/user/news/lib/python3.8/site-packages (from seaborn) (1.19.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/user/news/lib/python3.8/site-packages (from seaborn) (3.4.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/user/news/lib/python3.8/site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/user/news/lib/python3.8/site-packages (from seaborn) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/user/news/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user/news/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/user/news/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/user/news/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (8.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/news/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/user/news/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/news/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-27 18:48:38.189968: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-27 18:48:38.190077: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grpah-2300-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-27 18:49:00.508455: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-09-27 18:49:00.508567: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-09-27 18:49:00.538390: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-09-27 18:49:00.538832: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install nltk\n",
    "!pip install tensorflow\n",
    "!pip install seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv1D,MaxPooling1D , Conv2D,MaxPooling2D\n",
    "from keras.layers import LSTM,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import nltk \n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "Name = \"Grpah-2300-100\"\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='Logs2/{}'.format(Name))\n",
    "print(Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0973eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_data = pd.read_csv(\"News.csv\", header=None, skipinitialspace=True ,skiprows=1, encoding='utf-8') \n",
    "# # print a summary of the data in news_data\n",
    "# print(news_data[1])\n",
    "# news_data[1] = news_data[1].replace(['1-Septic', '0-Pure'], [1, 0])\n",
    "# news_data[0] = news_data[0].replace('\\n', '', regex=True).str.strip()\n",
    "# news_data.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b513f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels=news_data[1]\n",
    "# labels.head()     \n",
    "# print(type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2500c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_data[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d042a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = news_data[0]\n",
    "# Y=news_data[1]\n",
    "# x_train,x_test,y_train,y_test=train_test_split(X, Y, test_size=0.05, random_state=7)\n",
    "# print(type(x_test))\n",
    "# print(x_train.shape , y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb377aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbd17908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.str.replace('\\d+', '') # removing all numbers\n",
    "# x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28a53fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = x_test.str.replace('\\d+', '')\n",
    "# x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9273d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train.str.replace('[^\\w\\s\\]','') # remove all puctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d8d75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc6244e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = x_test.str.replace('[^\\w\\s\\\"]','')\n",
    "# x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c02498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/user/news/lib/python3.8/site-packages (3.6.2)\r\n",
      "Requirement already satisfied: regex in /home/user/news/lib/python3.8/site-packages (from nltk) (2021.8.3)\r\n",
      "Requirement already satisfied: click in /home/user/news/lib/python3.8/site-packages (from nltk) (8.0.1)\r\n",
      "Requirement already satisfied: tqdm in /home/user/news/lib/python3.8/site-packages (from nltk) (4.62.1)\r\n",
      "Requirement already satisfied: joblib in /home/user/news/lib/python3.8/site-packages (from nltk) (1.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a2bac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop = stopwords.words('english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f863a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b46420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = x_test.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "# x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4e396ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words=None) #adding this parameter can  responsible for setting the size of the vocabulary i.e the most common num_words\n",
    "# tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "#print(x_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# #DataFlair - Initialize a TfidfVectorizer\n",
    "# tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# #DataFlair - Fit and transform train set, transform test set\n",
    "# x_train=tfidf_vectorizer.fit_transform(x_train) \n",
    "# x_test=tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "# x_train=x_train.toarray()\n",
    "# x_test=x_test.toarray()\n",
    "# print(type(x_train))\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# x_train = cv.fit_transform(x_train).toarray()\n",
    "# x_test=cv.transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641ff27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c68eb5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(tfidf_vectorizer.vocabulary_)  \n",
    "# print(vocab_size)\n",
    "\n",
    "\n",
    "# vocab_size = len(tokenizer.word_index) + 1  \n",
    "# data = []\n",
    "# punc = '.'\n",
    "# for i in x_train:\n",
    "#     i = i[:-1]\n",
    "#     token = word_tokenize(i)\n",
    "#     if punc in token:\n",
    "#         for index ,val in enumerate(token):\n",
    "#             if punc == val:\n",
    "#                 token.pop(index) \n",
    "# #     x_train = tokenizer.texts_to_sequences(token)           \n",
    "#     data.append(token)\n",
    "\n",
    "# x_train = tokenizer.texts_to_sequences(data) \n",
    "# x_test = tokenizer.texts_to_sequences(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f86d66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxlen = 100\n",
    "# x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "# x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "# print(x_train)\n",
    "# news_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e6bc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.array(y_train)\n",
    "# y_test = np.array(y_test)\n",
    "# print(x_train.shape , vocab_size)\n",
    "# print(type(y_test))\n",
    "# y_train = y_train.astype('float64')\n",
    "# y_test = y_test.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f9bbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn tools\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras preprocessing, models, evaluators\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Activation, Dropout, GlobalMaxPool1D, Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "\n",
    "import hyperas\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45b84468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    real_data=pd.read_csv(\"News.csv\", header=None, skipinitialspace=True ,skiprows=1, encoding='utf-8')\n",
    "    septic_data = pd.read_csv(\"SepticDataset.csv\", header=None, skipinitialspace=True ,skiprows=1, encoding='utf-8') \n",
    "    septic_data=septic_data.drop(0, axis=1)\n",
    "    septic_data=septic_data.drop(1, axis=1)\n",
    "    septic_data.rename(columns = {2: 0, 3: 1}, inplace = True)\n",
    "    #fake_data= fake_data.iloc[:,1:]\n",
    "    real_data[1] = real_data[1].replace(['1-Septic', '0-Pure'], [1, 0])\n",
    "    #septic_data[1] = septic_data[1].replace(['1-Septic', '0-Pure'], [1, 0])\n",
    "    \n",
    "    news_data= pd.concat([real_data, septic_data], axis=0)\n",
    "    \n",
    "    # print a summary of the data in news_data\n",
    "    #print(news_data[1])\n",
    "    #news_data[1] = news_data[1].replace(['1-Septic', '0-Pure'], [1, 0])\n",
    "    news_data[0] = news_data[0].replace('\\n', '', regex=True).str.strip()\n",
    "    #news_data.head(20)\n",
    "    print(news_data.head(20))\n",
    "    print(real_data.head(20))\n",
    "    print(septic_data.head(20))\n",
    "    \n",
    "    labels=news_data[1]\n",
    "    labels.head()     \n",
    "    #print(type(labels))\n",
    "    \n",
    "    #news_data[1].value_counts()\n",
    "    \n",
    "    X = news_data[0]\n",
    "    Y=news_data[1]\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X, Y, test_size=0.05, random_state=7)\n",
    "    #print(type(x_test))\n",
    "    #print(x_train.shape , y_train.shape)\n",
    "    \n",
    "    \n",
    "    x_train = x_train.str.replace('\\d+', '') # removing all numbers\n",
    "    #x_train\n",
    "    \n",
    "    x_test = x_test.str.replace('\\d+', '')\n",
    "    #x_test\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    x_train = x_train.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    #x_train\n",
    "    \n",
    "    x_test = x_test.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    #x_test\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=None) #adding this parameter can  responsible for setting the size of the vocabulary i.e the most common num_words\n",
    "    tokenizer.fit_on_texts(x_train)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1  \n",
    "    data = []\n",
    "    punc = '.'\n",
    "    for i in x_train:\n",
    "        i = i[:-1]\n",
    "        token = word_tokenize(i)\n",
    "        if punc in token:\n",
    "            for index ,val in enumerate(token):\n",
    "                if punc == val:\n",
    "                    token.pop(index) \n",
    "    #     x_train = tokenizer.texts_to_sequences(token)           \n",
    "        data.append(token)\n",
    "\n",
    "    x_train = tokenizer.texts_to_sequences(data) \n",
    "    x_test = tokenizer.texts_to_sequences(x_test)\n",
    "    \n",
    "    maxlen = 100\n",
    "    x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "    x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "    #print(x_train)\n",
    "    #news_data.info()\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    #print(x_train.shape , vocab_size)\n",
    "    #print(type(y_test))\n",
    "    \n",
    "    y_train = y_train.astype('float64')\n",
    "    y_test = y_test.astype('float64')\n",
    "    \n",
    "    print(('X_train shape:', x_train.shape))\n",
    "    print(('X_test shape:', x_test.shape))\n",
    "    print(('y_train shape:', y_train.shape))\n",
    "    print(('y_test shape:', y_test.shape))\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc36923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-tuner\n",
    "# data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "997cc5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    num_classes = 1 # y_train.shape[1]\n",
    "    num_tokens = 100 #x_train.shape[1]\n",
    "    print(num_classes)\n",
    "    print(num_tokens)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, output_dim = {{choice([16,32,60, 80, 100])}}, input_length = 100))\n",
    "    model.add(Dropout({{choice([0.3, 0.4, 0.5, 0.6])}}))\n",
    "    model.add(Conv1D(filters = {{choice([32,50, 60, 80])}}, kernel_size = {{choice([3,6,10,15,20,30])}}, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer={{choice(['adam', 'sgd'])}},\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    # Optional to log output from Keras\n",
    "    #csv_logger = keras.callbacks.CSVLogger('Logs/dl_model.log')\n",
    "    \n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=10,\n",
    "              verbose=0,\n",
    "              validation_split=0.1)\n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f91bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb484327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Conv1D, MaxPooling1D, Conv2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.embeddings import Embedding\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.text import Tokenizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import warnings\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.callbacks import TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import nltk\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import accuracy_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.wrappers.scikit_learn import KerasRegressor\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import cross_val_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import KFold\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.pipeline import Pipeline\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Embedding, LSTM, GRU, Activation, Dropout, GlobalMaxPool1D, Conv1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.embeddings import Embedding\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.optimizers import SGD\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils.np_utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing import text, sequence\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import load_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import hyperas\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nltk.corpus import stopwords\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import logging\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import f1_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import precision_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import recall_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import seaborn as sns\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'output_dim': hp.choice('output_dim', [16,32,60, 80, 100]),\n",
      "        'Dropout': hp.choice('Dropout', [0.3, 0.4, 0.5, 0.6]),\n",
      "        'filters': hp.choice('filters', [32,50, 60, 80]),\n",
      "        'kernel_size': hp.choice('kernel_size', [3,6,10,15,20,30]),\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'sgd']),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: real_data=pd.read_csv(\"News.csv\", header=None, skipinitialspace=True ,skiprows=1, encoding='utf-8')\n",
      "   3: septic_data = pd.read_csv(\"SepticDataset.csv\", header=None, skipinitialspace=True ,skiprows=1, encoding='utf-8') \n",
      "   4: septic_data=septic_data.drop(0, axis=1)\n",
      "   5: septic_data=septic_data.drop(1, axis=1)\n",
      "   6: septic_data.rename(columns = {2: 0, 3: 1}, inplace = True)\n",
      "   7: #fake_data= fake_data.iloc[:,1:]\n",
      "   8: real_data[1] = real_data[1].replace(['1-Septic', '0-Pure'], [1, 0])\n",
      "   9: #septic_data[1] = septic_data[1].replace(['1-Septic', '0-Pure'], [1, 0])\n",
      "  10: \n",
      "  11: news_data= pd.concat([real_data, septic_data], axis=0)\n",
      "  12: \n",
      "  13: # print a summary of the data in news_data\n",
      "  14: #print(news_data[1])\n",
      "  15: #news_data[1] = news_data[1].replace(['1-Septic', '0-Pure'], [1, 0])\n",
      "  16: news_data[0] = news_data[0].replace('\\n', '', regex=True).str.strip()\n",
      "  17: #news_data.head(20)\n",
      "  18: print(news_data.head(20))\n",
      "  19: print(real_data.head(20))\n",
      "  20: print(septic_data.head(20))\n",
      "  21: \n",
      "  22: labels=news_data[1]\n",
      "  23: labels.head()     \n",
      "  24: #print(type(labels))\n",
      "  25: \n",
      "  26: #news_data[1].value_counts()\n",
      "  27: \n",
      "  28: X = news_data[0]\n",
      "  29: Y=news_data[1]\n",
      "  30: x_train,x_test,y_train,y_test=train_test_split(X, Y, test_size=0.05, random_state=7)\n",
      "  31: #print(type(x_test))\n",
      "  32: #print(x_train.shape , y_train.shape)\n",
      "  33: \n",
      "  34: \n",
      "  35: x_train = x_train.str.replace('\\d+', '') # removing all numbers\n",
      "  36: #x_train\n",
      "  37: \n",
      "  38: x_test = x_test.str.replace('\\d+', '')\n",
      "  39: #x_test\n",
      "  40: \n",
      "  41: from nltk.corpus import stopwords\n",
      "  42: stop = stopwords.words('english')\n",
      "  43: \n",
      "  44: x_train = x_train.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
      "  45: #x_train\n",
      "  46: \n",
      "  47: x_test = x_test.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
      "  48: #x_test\n",
      "  49: \n",
      "  50: tokenizer = Tokenizer(num_words=None) #adding this parameter can  responsible for setting the size of the vocabulary i.e the most common num_words\n",
      "  51: tokenizer.fit_on_texts(x_train)\n",
      "  52: \n",
      "  53: vocab_size = len(tokenizer.word_index) + 1  \n",
      "  54: data = []\n",
      "  55: punc = '.'\n",
      "  56: for i in x_train:\n",
      "  57:     i = i[:-1]\n",
      "  58:     token = word_tokenize(i)\n",
      "  59:     if punc in token:\n",
      "  60:         for index ,val in enumerate(token):\n",
      "  61:             if punc == val:\n",
      "  62:                 token.pop(index) \n",
      "  63: #     x_train = tokenizer.texts_to_sequences(token)           \n",
      "  64:     data.append(token)\n",
      "  65: \n",
      "  66: x_train = tokenizer.texts_to_sequences(data) \n",
      "  67: x_test = tokenizer.texts_to_sequences(x_test)\n",
      "  68: \n",
      "  69: maxlen = 100\n",
      "  70: x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
      "  71: x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
      "  72: #print(x_train)\n",
      "  73: #news_data.info()\n",
      "  74: \n",
      "  75: y_train = np.array(y_train)\n",
      "  76: y_test = np.array(y_test)\n",
      "  77: \n",
      "  78: #print(x_train.shape , vocab_size)\n",
      "  79: #print(type(y_test))\n",
      "  80: \n",
      "  81: y_train = y_train.astype('float64')\n",
      "  82: y_test = y_test.astype('float64')\n",
      "  83: \n",
      "  84: print(('X_train shape:', x_train.shape))\n",
      "  85: print(('X_test shape:', x_test.shape))\n",
      "  86: print(('y_train shape:', y_train.shape))\n",
      "  87: print(('y_test shape:', y_test.shape))\n",
      "  88: \n",
      "  89: \n",
      "  90: \n",
      "  91: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     num_classes = 1 # y_train.shape[1]\n",
      "   4:     num_tokens = 100 #x_train.shape[1]\n",
      "   5:     print(num_classes)\n",
      "   6:     print(num_tokens)\n",
      "   7:     model = Sequential()\n",
      "   8:     model.add(Embedding(vocab_size, output_dim = space['output_dim'], input_length = 100))\n",
      "   9:     model.add(Dropout(space['Dropout']))\n",
      "  10:     model.add(Conv1D(filters = space['filters'], kernel_size = space['kernel_size'], padding='valid', activation='relu', strides=1))\n",
      "  11:     model.add(GlobalMaxPool1D())\n",
      "  12:     model.add(Dense(num_classes))\n",
      "  13:     model.add(Activation('sigmoid'))\n",
      "  14: \n",
      "  15:     model.compile(loss='binary_crossentropy',\n",
      "  16:                   optimizer=space['optimizer'],\n",
      "  17:                   metrics=['acc'])\n",
      "  18: \n",
      "  19:     # Optional to log output from Keras\n",
      "  20:     #csv_logger = keras.callbacks.CSVLogger('Logs/dl_model.log')\n",
      "  21:     \n",
      "  22:     result = model.fit(x_train, y_train,\n",
      "  23:               batch_size=128,\n",
      "  24:               epochs=10,\n",
      "  25:               verbose=0,\n",
      "  26:               validation_split=0.1)\n",
      "  27:     #get the highest validation accuracy of the training epochs\n",
      "  28:     validation_acc = np.amax(result.history['val_acc']) \n",
      "  29:     print('Best validation acc of epoch:', validation_acc)\n",
      "  30:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      "  31: \n",
      "                                                    0  1\n",
      "0   The political predicament also appears to be t...  0\n",
      "1   The CM had planned to meet the duo when he com...  0\n",
      "2   He’s now scheduled to visit New Delhi on Thurs...  0\n",
      "3   “If he fails to do so, he would face open rebe...  0\n",
      "4   “Further delay is likely. Despite threats from...  0\n",
      "5   “The party should make a concerted effort to i...  0\n",
      "6   Taking serious note of allegations pertaining ...  0\n",
      "7   Panchayat raj and mining minister Pedireddy Ra...  0\n",
      "8   Sources said the party legislator reportedly s...  0\n",
      "9   The legislator is his report said that large q...  0\n",
      "10  The legislator claimed that the gravel was ext...  0\n",
      "11  “The Mandal Parishad Development Officers (MPD...  0\n",
      "12  He submitted also certain documents to substan...  0\n",
      "13  “The MPDOs requisitioned for supply of about 8...  0\n",
      "14  He said that massive illegal mining had not on...  0\n",
      "15  The legislator said that field level officials...  0\n",
      "16  Curiously, an MPDO, who is suspected to have p...  0\n",
      "17  The Indian government has set up an inquiry in...  0\n",
      "18  The issue was raised in the ongoing Parliament...  0\n",
      "19  Foreign minister S Jaishankar, in a letter to ...  0\n",
      "                                                    0  1\n",
      "0   The political predicament also appears to be t...  0\n",
      "1   The CM had planned to meet the duo when he com...  0\n",
      "2   He’s now scheduled to visit New Delhi on Thurs...  0\n",
      "3   “If he fails to do so, he would face open rebe...  0\n",
      "4   “Further delay is likely. Despite threats from...  0\n",
      "5   “The party should make a concerted effort to i...  0\n",
      "6   Taking serious note of allegations pertaining ...  0\n",
      "7   Panchayat raj and mining minister Pedireddy Ra...  0\n",
      "8   Sources said the party legislator reportedly s...  0\n",
      "9   The legislator is his report said that large q...  0\n",
      "10  The legislator claimed that the gravel was ext...  0\n",
      "11  “The Mandal Parishad Development Officers (MPD...  0\n",
      "12  He submitted also certain documents to substan...  0\n",
      "13  “The MPDOs requisitioned for supply of about 8...  0\n",
      "14  He said that massive illegal mining had not on...  0\n",
      "15  The legislator said that field level officials...  0\n",
      "16  Curiously, an MPDO, who is suspected to have p...  0\n",
      "17  The Indian government has set up an inquiry in...  0\n",
      "18  The issue was raised in the ongoing Parliament...  0\n",
      "19  Foreign minister S Jaishankar, in a letter to ...  0\n",
      "                                                    0  1\n",
      "0   fared bangkok fifth dy slow stripes enjoying a...  1\n",
      "1   mela comorbidities check introspection junior ...  1\n",
      "2   moves coupe guide championship downplayed text...  1\n",
      "3   pradeep yogaraj hub jonny spreading members co...  1\n",
      "4   gaekwad vacancies sena principals devendra boo...  1\n",
      "5   surprising shared supreme administrative laws ...  1\n",
      "6   farmer staff kushwaha par getting iits reach e...  1\n",
      "7   findings commented depend depend based heavy k...  1\n",
      "8   stars punjabi civet triggered nation utah inst...  1\n",
      "9   missile alexander extended petroleum farm orga...  1\n",
      "10  reservoir loved deepak blueprint hc outside pr...  1\n",
      "11  ominous war harassed minute political month re...  1\n",
      "12  raised fixtures city spokesperson disaster hou...  1\n",
      "13  statement yadav internet march convene politic...  1\n",
      "14  focus analarm coins ordinances learnt project ...  1\n",
      "15  person tactical building pathan overly october...  1\n",
      "16  zubeen kinetic north style know sharpen warner...  1\n",
      "17  died puthuppally change role run division team...  1\n",
      "18  naga remember cats stronger person heartening ...  1\n",
      "19  app underperformance harikrishna host greatest...  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (7026, 100))\n",
      "('X_test shape:', (370, 100))\n",
      "('y_train shape:', (7026,))\n",
      "('y_test shape:', (370,))\n",
      "1                                                                               \n",
      "100                                                                             \n",
      "Best validation acc of epoch:                                                   \n",
      "0.9487909078598022                                                              \n",
      "1                                                                               \n",
      "100                                                                             \n",
      "Best validation acc of epoch:                                                   \n",
      "0.941678524017334                                                               \n",
      "1                                                                               \n",
      "100                                                                             \n",
      "Best validation acc of epoch:                                                   \n",
      "0.9502133727073669                                                              \n",
      "1                                                                               \n",
      "100                                                                             \n",
      "Best validation acc of epoch:                                                   \n",
      "0.9559032917022705                                                              \n",
      "1                                                                               \n",
      "100                                                                             \n",
      "Best validation acc of epoch:                                                   \n",
      "0.9530583024024963                                                              \n",
      "100%|██████████| 5/5 [02:01<00:00, 24.28s/trial, best loss: -0.9559032917022705]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "try:\n",
    "    best_run, best_model, space = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='SepticClassification',\n",
    "                                          eval_space=True,   # <-- this is the line that puts real values into 'best_run'\n",
    "                                          return_space=True,  # <-- this allows you to save the space for later evaluations\n",
    "                                          )\n",
    "except Exception as e:\n",
    "    logging.exception(\"message\")\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import layers\n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# def add_conv_block(model):\n",
    "#     model.add(layers.Embedding(vocab_size, 500, input_length=vocab_size))\n",
    "#     model.add(layers.Conv1D(16, 5, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(layers.GlobalMaxPooling1D())\n",
    "#     model.add(Dropout(0.5))\n",
    "#     return model\n",
    "# model = Sequential()\n",
    "# model=add_conv_block(model)   \n",
    "# model=add_conv_block(model)\n",
    "\n",
    "# model.add(layers.Dense(10, activation='relu'))\n",
    "# #model.add(layers.Dense(5, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# X=np.array(x_train)\n",
    "# Y=np.array(y_train)\n",
    "# model.fit(X, Y,epochs=10,batch_size=10 , validation_split=0.1 , callbacks= [tensorboard])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from keras import layers\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(layers.Embedding(vocab_size, 16, input_length=100))\n",
    "# model.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "\n",
    "# model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# X=np.array(x_train)\n",
    "# Y=np.array(y_train)\n",
    "# model.fit(X, Y,epochs=30,batch_size=10 , validation_split=0.1 , callbacks= [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cf25d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "# #DataFlair - Initialize a PassiveAggressiveClassifier\n",
    "# pac=PassiveAggressiveClassifier(max_iter=50)\n",
    "# pac.fit(x_train,y_train)\n",
    "\n",
    "# #DataFlair - Predict on the test set and calculate accuracy\n",
    "# y_pred=pac.predict(x_test)\n",
    "# score=accuracy_score(y_test,y_pred)\n",
    "# print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68c6d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0  1\n",
      "0   The political predicament also appears to be t...  0\n",
      "1   The CM had planned to meet the duo when he com...  0\n",
      "2   He’s now scheduled to visit New Delhi on Thurs...  0\n",
      "3   “If he fails to do so, he would face open rebe...  0\n",
      "4   “Further delay is likely. Despite threats from...  0\n",
      "5   “The party should make a concerted effort to i...  0\n",
      "6   Taking serious note of allegations pertaining ...  0\n",
      "7   Panchayat raj and mining minister Pedireddy Ra...  0\n",
      "8   Sources said the party legislator reportedly s...  0\n",
      "9   The legislator is his report said that large q...  0\n",
      "10  The legislator claimed that the gravel was ext...  0\n",
      "11  “The Mandal Parishad Development Officers (MPD...  0\n",
      "12  He submitted also certain documents to substan...  0\n",
      "13  “The MPDOs requisitioned for supply of about 8...  0\n",
      "14  He said that massive illegal mining had not on...  0\n",
      "15  The legislator said that field level officials...  0\n",
      "16  Curiously, an MPDO, who is suspected to have p...  0\n",
      "17  The Indian government has set up an inquiry in...  0\n",
      "18  The issue was raised in the ongoing Parliament...  0\n",
      "19  Foreign minister S Jaishankar, in a letter to ...  0\n",
      "                                                    0  1\n",
      "0   The political predicament also appears to be t...  0\n",
      "1   The CM had planned to meet the duo when he com...  0\n",
      "2   He’s now scheduled to visit New Delhi on Thurs...  0\n",
      "3   “If he fails to do so, he would face open rebe...  0\n",
      "4   “Further delay is likely. Despite threats from...  0\n",
      "5   “The party should make a concerted effort to i...  0\n",
      "6   Taking serious note of allegations pertaining ...  0\n",
      "7   Panchayat raj and mining minister Pedireddy Ra...  0\n",
      "8   Sources said the party legislator reportedly s...  0\n",
      "9   The legislator is his report said that large q...  0\n",
      "10  The legislator claimed that the gravel was ext...  0\n",
      "11  “The Mandal Parishad Development Officers (MPD...  0\n",
      "12  He submitted also certain documents to substan...  0\n",
      "13  “The MPDOs requisitioned for supply of about 8...  0\n",
      "14  He said that massive illegal mining had not on...  0\n",
      "15  The legislator said that field level officials...  0\n",
      "16  Curiously, an MPDO, who is suspected to have p...  0\n",
      "17  The Indian government has set up an inquiry in...  0\n",
      "18  The issue was raised in the ongoing Parliament...  0\n",
      "19  Foreign minister S Jaishankar, in a letter to ...  0\n",
      "                                                    0  1\n",
      "0   fared bangkok fifth dy slow stripes enjoying a...  1\n",
      "1   mela comorbidities check introspection junior ...  1\n",
      "2   moves coupe guide championship downplayed text...  1\n",
      "3   pradeep yogaraj hub jonny spreading members co...  1\n",
      "4   gaekwad vacancies sena principals devendra boo...  1\n",
      "5   surprising shared supreme administrative laws ...  1\n",
      "6   farmer staff kushwaha par getting iits reach e...  1\n",
      "7   findings commented depend depend based heavy k...  1\n",
      "8   stars punjabi civet triggered nation utah inst...  1\n",
      "9   missile alexander extended petroleum farm orga...  1\n",
      "10  reservoir loved deepak blueprint hc outside pr...  1\n",
      "11  ominous war harassed minute political month re...  1\n",
      "12  raised fixtures city spokesperson disaster hou...  1\n",
      "13  statement yadav internet march convene politic...  1\n",
      "14  focus analarm coins ordinances learnt project ...  1\n",
      "15  person tactical building pathan overly october...  1\n",
      "16  zubeen kinetic north style know sharpen warner...  1\n",
      "17  died puthuppally change role run division team...  1\n",
      "18  naga remember cats stronger person heartening ...  1\n",
      "19  app underperformance harikrishna host greatest...  1\n",
      "('X_train shape:', (7026, 100))\n",
      "('X_test shape:', (370, 100))\n",
      "('y_train shape:', (7026,))\n",
      "('y_test shape:', (370,))\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.1574 - acc: 0.9459\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = data() \n",
    "acc , y = model.evaluate(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55bac14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91297a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mapper(a):\n",
    "#     a = tokenizer.texts_to_sequences(a)\n",
    "#     return pad_sequences(a, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "# x_eval = mapper(['he said, \"the world goes round and round and round\"',\n",
    "#         'Even Bangladesh leader Khaleda Zia has said the terrorists are being trained in India and sent to Bangladesh to create disturbance'])\n",
    "\n",
    "# y_eval = np.array([0, 1])\n",
    "# model.evaluate(x_eval, y_eval)\n",
    "\n",
    "# for i in model.predict(x_eval):\n",
    "#     print(round(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "564a230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = [round(i[0]) for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07b9b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 87   8]\n",
      " [ 12 263]]\n",
      "Accuracy: 94.59%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y = confusion_matrix(y_test, y_pred)\n",
    "print(y)\n",
    "score=accuracy_score(y_test,y_pred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07f20bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Measure: 0.963\n",
      "Precision: 0.970\n",
      "Recall: 0.956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "score = f1_score(y_test, y_pred, average='binary')\n",
    "print('F-Measure: %.3f' % score)\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "print('Precision: %.3f' % precision)\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "print('Recall: %.3f' % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88d80003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD4CAYAAADSIzzWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAANPUlEQVR4nO3dX6hl5XnH8e9Pa3NRhSjSYRynaJOxYbyohqkNpBcWof65meRGxkA6hIGTCwci5CKT9CLphZBCNBBahBMUFVKnA0lwEElrhqRWGqOTMkwcp+JgFOdkdGgtiSUl8Zz99OIsye70nL33OWfPeWev+X5kcfZ+99prvRfDz4dnvWvtVBWSpM13SesJSNLFygCWpEYMYElqxACWpEYMYElq5HfO9wn+6rpPucxC/8/f/PyfW09BF6DF3yxko8d47z9emzhzLrv6Dzd8vo047wEsSZtqsNR6BhMzgCX1Sw1az2BiBrCkfhkYwJLURFkBS1IjS4utZzAxA1hSv3gRTpIamaEWhDdiSOqXwWDybYQk25P8IMnLSU4k+Vw3/pUkC0mOddtdQ9/5YpJTSV5Jcvu4qVoBS+qVKV6EWwQ+X1X/luQK4CdJnuk++3pVfW145yQ7gT3AjcA1wPeT3FBVq/ZEDGBJ/TKlZWhVdQY4071+N8lJYNuIr+wGDlbVr4GfJTkF3AL8aLUv2IKQ1C9L7028JZlLcnRom1vpkEmuA24GftwN7U9yPMkjSa7sxrYBbw597TSjA9sAltQzNZh4q6r5qto1tM2fe7gklwPfBu6rql8CDwEfAm5iuUJ+YL1TtQUhqV+meCdckstYDt9vVdV3AKrq7aHPvwk81b1dALYPff3abmxVVsCS+mUNFfAoSQI8DJysqgeHxrcO7fZJ4KXu9WFgT5IPJLke2AG8MOocVsCS+mV6FfDHgU8DP01yrBv7EnBPkpuAAl4HPgtQVSeSHAJeZnkFxb2jVkCAASypZ2rw3nSOU/UcsNLzgp8e8Z37gfsnPYcBLKlffBqaJDUyQ7ciG8CS+sWH8UhSI1bAktSIPWBJasQHsktSI1bAktTGmHsfLigGsKR+sQKWpEZcBSFJjVgBS1IjroKQpEZsQUhSI7YgJKkRA1iSGrEFIUmNeBFOkhqxBSFJjdiCkKRGrIAlqREDWJIaqWo9g4kZwJL6ZdFVEJLUhhfhJKkRe8CS1Ig9YElqxApYkhoxgCWpjVryRzklqQ0rYElqxGVoktTIwFUQktSGLQhJamSGLsJd0noCkjRVg8Hk2whJtif5QZKXk5xI8rlu/KokzyR5tft7ZTeeJN9IcirJ8SQfHTdVA1hSvwxq8m20ReDzVbUT+Bhwb5KdwAHgSFXtAI507wHuBHZ02xzw0LgTjG1BJPkIsBvY1g0tAIer6uS470rSppvSKoiqOgOc6V6/m+Qkyzm4G7i12+0x4IfAF7rxx6uqgOeTfDDJ1u44KxpZASf5AnAQCPBCtwV4IsmBUd+VpCbWUAEnmUtydGibW+mQSa4DbgZ+DGwZCtW3gC3d623Am0NfO81vC9cVjauA9wE3VtV750zmQeAE8NVVJjvHcgnOnVf9CTdf8eExp5Gk6ag1rIKoqnlgftQ+SS4Hvg3cV1W/TDL8/Uqy7nVv43rAA+CaFca3dp+tqKrmq2pXVe0yfCVtqqWlybcxklzGcvh+q6q+0w2/nWRr9/lW4Gw3vgBsH/r6td3YqsZVwPcBR5K8ym9L6z8APgzsHzt7SdpsU7oRI8ul7sPAyap6cOijw8BeljsAe4Enh8b3JzkI/Cnwi1H9XxgTwFX1vSQ3ALfwfy/CvVhVs7PYTtLFY3o3Ynwc+DTw0yTHurEvsRy8h5LsA94A7u4+exq4CzgF/Ar4zLgTjF0FUVUD4Pm1zlySmphSBVxVz7G86GAlt62wfwH3ruUc3gknqV98GI8kNeLDeCSpjVqcnctTBrCkfrEClqRG7AFLUiNWwJLURhnAktSIF+EkqRErYElqxACWpDaW7wieDQawpH6xApakRgxgSWqjFr0RQ5LamJ38NYAl9Ys3YkhSKwawJDViC0KS2rAFIUmN1KIBLElt2IKQpDZm6HnsBrCknjGAJakNK2BJaqQWW89gcgawpF6xApakRgxgSWql0noGEzOAJfWKFbAkNVIDK2BJamKwZABLUhO2ICSpEVsQktTIDP0qPZe0noAkTVMNMvE2TpJHkpxN8tLQ2FeSLCQ51m13DX32xSSnkryS5PZxx7cCltQrU74I9yjwt8Dj54x/vaq+NjyQZCewB7gRuAb4fpIbqmpptYNbAUvqlWlWwFX1LPDOhKfeDRysql9X1c+AU8Ato75gAEvqlapMvCWZS3J0aJub8DT7kxzvWhRXdmPbgDeH9jndja3KAJbUKzVYw1Y1X1W7hrb5CU7xEPAh4CbgDPDAeudqD1hSrwzO87Mgqurt918n+SbwVPd2Adg+tOu13diqrIAl9cpaWhDrkWTr0NtPAu+vkDgM7EnygSTXAzuAF0YdywpYUq9McxVEkieAW4Grk5wGvgzcmuQmoIDXgc8CVNWJJIeAl4FF4N5RKyDAAJbUM9O8E66q7llh+OER+98P3D/p8Q1gSb1yvnvA02QAS+qV9fZ2WzCAJfXKLD0LwgCW1Cu2ICSpkYGPo5SkNqyAhzzw1nPn+xSaQf/z839pPQX1lBfhJKkRK2BJamSGFkEYwJL6ZWkwO4+4MYAl9coM/SiyASypXwp7wJLUxGCGmsAGsKReGVgBS1IbtiAkqZElA1iS2nAVhCQ1YgBLUiP2gCWpkRl6GqUBLKlfXIYmSY2M/B34C4wBLKlXBrEClqQmZuhOZANYUr+4DE2SGnEVhCQ14q3IktSIFbAkNWIPWJIacRWEJDViC0KSGrEFIUmNLM1QBXxJ6wlI0jQN1rCNk+SRJGeTvDQ0dlWSZ5K82v29shtPkm8kOZXkeJKPjju+ASypV6YZwMCjwB3njB0AjlTVDuBI9x7gTmBHt80BD407uAEsqVdqDdvYY1U9C7xzzvBu4LHu9WPAJ4bGH69lzwMfTLJ11PENYEm9MsjkW5K5JEeHtrkJTrGlqs50r98CtnSvtwFvDu13uhtblRfhJPXKWlZBVNU8ML/ec1VVJVn30mMDWFKvbMID2d9OsrWqznQthrPd+AKwfWi/a7uxVdmCkNQra2lBrNNhYG/3ei/w5ND4X3arIT4G/GKoVbEiK2BJvTLNGzGSPAHcClyd5DTwZeCrwKEk+4A3gLu73Z8G7gJOAb8CPjPu+AawpF6Z5rMgquqeVT66bYV9C7h3Lcc3gCX1ymCGHsdjAEvqFX8VWZIa8WE8ktSIj6OUpEbsAUtSI7MTvwawpJ6xByxJjSzNUA1sAEvqFStgSWrEi3CS1MjsxK8BLKlnbEFIUiNehJOkRuwBS1IjsxO/BrCknrEClqRGvAgnSY3UDFXA6/5RziSr/t5RkrkkR5McXVr67/WeQpLWbImaeGttI7+K/NerfVBV81W1q6p2XXrp5Rs4hSStzWANW2sjWxBJjq/2EbBl+tORpI0ZVPvKdlLjesBbgNuB/zpnPMC/npcZSdIGzE78jg/gp4DLq+rYuR8k+eH5mJAkbURvlqFV1b4Rn31q+tORpI2ZpVUQLkOT1CuLBrAktWEFLEmNXAjLyyZlAEvqlerRMjRJmim9WQUhSbPmQrjFeFIGsKResQKWpEbsAUtSI66CkKRGXAcsSY1Mswec5HXgXWAJWKyqXUmuAv4BuA54Hbi7qs59YNlENvI8YEm64CzVYOJtQn9eVTdV1a7u/QHgSFXtAI5079fFAJbUK7WG/9ZpN/BY9/ox4BPrPZABLKlXBlUTb8M/n9Ztc+ccroB/SvKToc+2VNWZ7vVbbODHKewBS+qVtdS1VTUPzI/Y5c+qaiHJ7wPPJPn3c75fSdZdSlsBS+qVATXxNk5VLXR/zwLfBW4B3k6yFaD7e3a9czWAJfXKtAI4ye8lueL918BfAC8Bh4G93W57gSfXO1dbEJJ6ZQ2rG8bZAnw3CSxn5d9X1feSvAgcSrIPeAO4e70nMIAl9cq0bsSoqteAP15h/D+B26ZxDgNYUq/4LAhJasSnoUlSI1bAktTI0gw9D80AltQrAytgSWrDx1FKUiNWwJLUiBWwJDViBSxJjUzxVuTzzgCW1Cu2ICSpkbIClqQ2vBVZkhrxVmRJasQKWJIaWRrYA5akJlwFIUmN2AOWpEbsAUtSI1bAktSIF+EkqRFbEJLUiC0ISWrEx1FKUiOuA5akRqyAJamRgY+jlKQ2vAgnSY0YwJLUyOzEL2SW/m8x65LMVdV863nowuK/i4vXJa0ncJGZaz0BXZD8d3GRMoAlqREDWJIaMYA3l30+rcR/FxcpL8JJUiNWwJLUiAEsSY0YwJskyR1JXklyKsmB1vNRe0keSXI2yUut56I2DOBNkORS4O+AO4GdwD1JdradlS4AjwJ3tJ6E2jGAN8ctwKmqeq2qfgMcBHY3npMaq6pngXdaz0PtGMCbYxvw5tD7092YpIuYASxJjRjAm2MB2D70/tpuTNJFzADeHC8CO5Jcn+R3gT3A4cZzktSYAbwJqmoR2A/8I3ASOFRVJ9rOSq0leQL4EfBHSU4n2dd6Ttpc3oosSY1YAUtSIwawJDViAEtSIwawJDViAEtSIwawJDViAEtSI/8Lpzf2eZrCx7cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62c35f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 32)           337824    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 86, 32)            15392     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 353,249\n",
      "Trainable params: 353,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34ff9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "filename='model.h5'\n",
    "tf.keras.models.save_model(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "584e984d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x7f45cc7849d0>\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.models.load_model('model.h5')\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d884d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03f2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d188e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aaa438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339fdda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
