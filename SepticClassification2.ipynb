{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "026738ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/user/news/lib/python3.8/site-packages (2.6.0)\n",
      "Requirement already satisfied: nltk in /home/user/news/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: regex in /home/user/news/lib/python3.8/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: click in /home/user/news/lib/python3.8/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in /home/user/news/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /home/user/news/lib/python3.8/site-packages (from nltk) (4.62.1)\n",
      "Requirement already satisfied: tensorflow in /home/user/news/lib/python3.8/site-packages (2.6.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (0.37.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.39.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: clang~=5.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (5.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: keras~=2.6 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /home/user/news/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (44.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.4.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.34.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/user/news/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/user/news/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/news/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/news/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/user/news/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/user/news/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/user/news/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/user/news/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/user/news/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/user/news/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/user/news/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: seaborn in /home/user/news/lib/python3.8/site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/user/news/lib/python3.8/site-packages (from seaborn) (1.19.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/user/news/lib/python3.8/site-packages (from seaborn) (3.4.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/user/news/lib/python3.8/site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/user/news/lib/python3.8/site-packages (from seaborn) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/user/news/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/user/news/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/user/news/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (8.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/news/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user/news/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/user/news/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/news/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.15.0)\n",
      "Grpah-2300-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-27 19:37:05.343795: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-10-27 19:37:05.343911: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-10-27 19:37:05.344836: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install nltk\n",
    "!pip install tensorflow\n",
    "!pip install seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv1D,MaxPooling1D , Conv2D,MaxPooling2D\n",
    "from keras.layers import LSTM,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import nltk \n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "Name = \"Grpah-2300-100\"\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='Logs2/{}'.format(Name))\n",
    "print(Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "0973eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_data = pd.read_csv(\"News.csv\", header=None, skipinitialspace=True ,skiprows=1, encoding='utf-8') \n",
    "# # print a summary of the data in news_data\n",
    "# print(news_data[1])\n",
    "# news_data[1] = news_data[1].replace(['1-Septic', '0-Pure'], [1, 0])\n",
    "# news_data[0] = news_data[0].replace('\\n', '', regex=True).str.strip()\n",
    "# news_data.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "b513f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels=news_data[1]\n",
    "# labels.head()     \n",
    "# print(type(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "2500c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_data[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "2d042a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = news_data[0]\n",
    "# Y=news_data[1]\n",
    "# x_train,x_test,y_train,y_test=train_test_split(X, Y, test_size=0.05, random_state=7)\n",
    "# print(type(x_test))\n",
    "# print(x_train.shape , y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "1eb377aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "dbd17908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.str.replace('\\d+', '') # removing all numbers\n",
    "# x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "28a53fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = x_test.str.replace('\\d+', '')\n",
    "# x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "c9273d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = x_train.str.replace('[^\\w\\s\\]','') # remove all puctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "3d8d75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "bc6244e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = x_test.str.replace('[^\\w\\s\\\"]','')\n",
    "# x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "2c02498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/user/news/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: joblib in /home/user/news/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /home/user/news/lib/python3.8/site-packages (from nltk) (4.62.1)\n",
      "Requirement already satisfied: click in /home/user/news/lib/python3.8/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: regex in /home/user/news/lib/python3.8/site-packages (from nltk) (2021.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "9a2bac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop = stopwords.words('english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "f863a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "# x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "3b46420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = x_test.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "# x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "a4e396ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words=None) #adding this parameter can  responsible for setting the size of the vocabulary i.e the most common num_words\n",
    "# tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "#print(x_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# #DataFlair - Initialize a TfidfVectorizer\n",
    "# tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# #DataFlair - Fit and transform train set, transform test set\n",
    "# x_train=tfidf_vectorizer.fit_transform(x_train) \n",
    "# x_test=tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "# x_train=x_train.toarray()\n",
    "# x_test=x_test.toarray()\n",
    "# print(type(x_train))\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# x_train = cv.fit_transform(x_train).toarray()\n",
    "# x_test=cv.transform(x_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e641ff27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "c68eb5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = len(tfidf_vectorizer.vocabulary_)  \n",
    "# print(vocab_size)\n",
    "\n",
    "\n",
    "# vocab_size = len(tokenizer.word_index) + 1  \n",
    "# data = []\n",
    "# punc = '.'\n",
    "# for i in x_train:\n",
    "#     i = i[:-1]\n",
    "#     token = word_tokenize(i)\n",
    "#     if punc in token:\n",
    "#         for index ,val in enumerate(token):\n",
    "#             if punc == val:\n",
    "#                 token.pop(index) \n",
    "# #     x_train = tokenizer.texts_to_sequences(token)           \n",
    "#     data.append(token)\n",
    "\n",
    "# x_train = tokenizer.texts_to_sequences(data) \n",
    "# x_test = tokenizer.texts_to_sequences(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "f86d66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxlen = 100\n",
    "# x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "# x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "# print(x_train)\n",
    "# news_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "2e6bc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = np.array(y_train)\n",
    "# y_test = np.array(y_test)\n",
    "# print(x_train.shape , vocab_size)\n",
    "# print(type(y_test))\n",
    "# y_train = y_train.astype('float64')\n",
    "# y_test = y_test.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "6f9bbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn tools\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras preprocessing, models, evaluators\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Activation, Dropout, GlobalMaxPool1D, Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "from keras.models import load_model\n",
    "\n",
    "import hyperas\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "45b84468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    real_data=pd.read_csv(\"News.csv\", header=None, skipinitialspace=True ,skiprows=1, encoding='utf-8')\n",
    "    real_data[1] = real_data[1].replace(['1-Septic', '0-Pure'], [1, 0])\n",
    "    real_data[0] = real_data[0].replace('\\n', '', regex=True).str.strip()  \n",
    "    #labels=news_data[1]\n",
    "    #labels.head()     \n",
    "    #print(type(labels))\n",
    "    \n",
    "    #news_data[1].value_counts()\n",
    "    \n",
    "    X = real_data[0]\n",
    "    Y=real_data[1]\n",
    "    #x1=X.iloc[0:100]\n",
    "    #x2=X.iloc[-100:]\n",
    "    #y1=Y.iloc[0:100]\n",
    "    #y2=Y.iloc[-100:]\n",
    "    #x_train=X.iloc[100:-100]\n",
    "    #y_train=Y.iloc[100:-100]\n",
    "    #x_test=pd.concat([x1, x2], axis=0)\n",
    "    #y_test=pd.concat([y1, y2], axis=0)\n",
    "    x_train,x_test,y_train,y_test=train_test_split(X, Y, test_size=0.1, random_state=7)\n",
    "    #print(type(x_test))\n",
    "    #print(x_train.shape , y_train.shape)\n",
    "    septic_data = pd.read_csv(\"SepticDataset.csv\", header=None, skipinitialspace=True ,skiprows=1, encoding='utf-8') \n",
    "    septic_data=septic_data.drop(0, axis=1)\n",
    "    septic_data=septic_data.drop(1, axis=1)\n",
    "    septic_data.rename(columns = {2: 0, 3: 1}, inplace = True)\n",
    "    #septic_data=septic_data.iloc[0:1500,:]\n",
    "    #news_data= pd.concat([real_data, septic_data], axis=0)\n",
    "    septic_data[0] = septic_data[0].replace('\\n', '', regex=True).str.strip()\n",
    "    \n",
    "    x_train=pd.concat([x_train, septic_data[0]], axis=0)\n",
    "    y_train=pd.concat([y_train, septic_data[1]], axis=0)\n",
    "    #print(news_data.head(20))\n",
    "    #print(real_data.head(20))\n",
    "    #print(septic_data.head(20))\n",
    "  \n",
    "    \n",
    "    x_train = x_train.str.replace('\\d+', '') # removing all numbers\n",
    "    #x_train\n",
    "    \n",
    "    x_test = x_test.str.replace('\\d+', '')\n",
    "    #x_test\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    x_train = x_train.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    #x_train\n",
    "    \n",
    "    x_test = x_test.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    #x_test\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=None) #adding this parameter can  responsible for setting the size of the vocabulary i.e the most common num_words\n",
    "    tokenizer.fit_on_texts(x_train)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1  \n",
    "    data = []\n",
    "    punc = '.'\n",
    "    for i in x_train:\n",
    "        i = i[:-1]\n",
    "        token = word_tokenize(i)\n",
    "        if punc in token:\n",
    "            for index ,val in enumerate(token):\n",
    "                if punc == val:\n",
    "                    token.pop(index) \n",
    "    #     x_train = tokenizer.texts_to_sequences(token)           \n",
    "        data.append(token)\n",
    "\n",
    "    x_train = tokenizer.texts_to_sequences(data) \n",
    "    x_test = tokenizer.texts_to_sequences(x_test)\n",
    "    \n",
    "    maxlen = 100\n",
    "    x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
    "    x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
    "    #print(x_train)\n",
    "    #news_data.info()\n",
    "    \n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    #print(x_train.shape , vocab_size)\n",
    "    #print(type(y_test))\n",
    "    \n",
    "    y_train = y_train.astype('float64')\n",
    "    y_test = y_test.astype('float64')\n",
    "    \n",
    "    print(('X_train shape:', x_train.shape))\n",
    "    print(('X_test shape:', x_test.shape))\n",
    "    print(('y_train shape:', y_train.shape))\n",
    "    print(('y_test shape:', y_test.shape))\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "fc36923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-tuner\n",
    "# data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "997cc5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    num_classes = 1 # y_train.shape[1]\n",
    "    num_tokens = 100 #x_train.shape[1]\n",
    "    print(num_classes)\n",
    "    print(num_tokens)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, output_dim = {{choice([16,32,60, 80, 100])}}, input_length = 100))\n",
    "    model.add(Dropout({{choice([0.3, 0.4, 0.5, 0.6])}}))\n",
    "    model.add(Conv1D(filters = {{choice([32,50, 60, 80])}}, kernel_size = {{choice([3,6,10,15,20,30])}}, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer={{choice(['adam', 'sgd'])}},\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    # Optional to log output from Keras\n",
    "    #csv_logger = keras.callbacks.CSVLogger('Logs/dl_model.log')\n",
    "    \n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size=128,\n",
    "              epochs=10,\n",
    "              verbose=0,\n",
    "              validation_split=0.1)\n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_acc']) \n",
    "    print('Best validation acc of epoch:', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "2f91bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "cb484327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Conv1D, MaxPooling1D, Conv2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.embeddings import Embedding\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.text import Tokenizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import warnings\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.callbacks import TensorBoard\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import nltk\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.feature_extraction.text import CountVectorizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import accuracy_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.wrappers.scikit_learn import KerasRegressor\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import cross_val_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import KFold\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.pipeline import Pipeline\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Embedding, LSTM, GRU, Activation, Dropout, GlobalMaxPool1D, Conv1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.embeddings import Embedding\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.optimizers import SGD\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils.np_utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing import text, sequence\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import load_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import hyperas\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nltk.corpus import stopwords\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import logging\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import nltk\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import re\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from nltk.corpus import stopwords\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import classification_report, confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.svm import LinearSVC\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.pipeline import Pipeline\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.neural_network import MLPClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from joblib import dump, load\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.ensemble import GradientBoostingClassifier\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import tree\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import itertools\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'output_dim': hp.choice('output_dim', [16,32,60, 80, 100]),\n",
      "        'Dropout': hp.choice('Dropout', [0.3, 0.4, 0.5, 0.6]),\n",
      "        'filters': hp.choice('filters', [32,50, 60, 80]),\n",
      "        'kernel_size': hp.choice('kernel_size', [3,6,10,15,20,30]),\n",
      "        'optimizer': hp.choice('optimizer', ['adam', 'sgd']),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: real_data=pd.read_csv(\"News.csv\", header=None, skipinitialspace=True ,skiprows=1, encoding='utf-8')\n",
      "   3: real_data[1] = real_data[1].replace(['1-Septic', '0-Pure'], [1, 0])\n",
      "   4: real_data[0] = real_data[0].replace('\\n', '', regex=True).str.strip()  \n",
      "   5: #labels=news_data[1]\n",
      "   6: #labels.head()     \n",
      "   7: #print(type(labels))\n",
      "   8: \n",
      "   9: #news_data[1].value_counts()\n",
      "  10: \n",
      "  11: X = real_data[0]\n",
      "  12: Y=real_data[1]\n",
      "  13: #x1=X.iloc[0:100]\n",
      "  14: #x2=X.iloc[-100:]\n",
      "  15: #y1=Y.iloc[0:100]\n",
      "  16: #y2=Y.iloc[-100:]\n",
      "  17: #x_train=X.iloc[100:-100]\n",
      "  18: #y_train=Y.iloc[100:-100]\n",
      "  19: #x_test=pd.concat([x1, x2], axis=0)\n",
      "  20: #y_test=pd.concat([y1, y2], axis=0)\n",
      "  21: x_train,x_test,y_train,y_test=train_test_split(X, Y, test_size=0.1, random_state=7)\n",
      "  22: #print(type(x_test))\n",
      "  23: #print(x_train.shape , y_train.shape)\n",
      "  24: septic_data = pd.read_csv(\"SepticDataset1.csv\", header=None, skipinitialspace=True ,skiprows=1, encoding='utf-8') \n",
      "  25: septic_data=septic_data.drop(0, axis=1)\n",
      "  26: septic_data=septic_data.drop(1, axis=1)\n",
      "  27: septic_data.rename(columns = {2: 0, 3: 1}, inplace = True)\n",
      "  28: #septic_data=septic_data.iloc[0:1500,:]\n",
      "  29: #news_data= pd.concat([real_data, septic_data], axis=0)\n",
      "  30: septic_data[0] = septic_data[0].replace('\\n', '', regex=True).str.strip()\n",
      "  31: \n",
      "  32: x_train=pd.concat([x_train, septic_data[0]], axis=0)\n",
      "  33: y_train=pd.concat([y_train, septic_data[1]], axis=0)\n",
      "  34: #print(news_data.head(20))\n",
      "  35: #print(real_data.head(20))\n",
      "  36: #print(septic_data.head(20))\n",
      "  37: \n",
      "  38: \n",
      "  39: x_train = x_train.str.replace('\\d+', '') # removing all numbers\n",
      "  40: #x_train\n",
      "  41: \n",
      "  42: x_test = x_test.str.replace('\\d+', '')\n",
      "  43: #x_test\n",
      "  44: \n",
      "  45: from nltk.corpus import stopwords\n",
      "  46: stop = stopwords.words('english')\n",
      "  47: \n",
      "  48: x_train = x_train.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
      "  49: #x_train\n",
      "  50: \n",
      "  51: x_test = x_test.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
      "  52: #x_test\n",
      "  53: \n",
      "  54: tokenizer = Tokenizer(num_words=None) #adding this parameter can  responsible for setting the size of the vocabulary i.e the most common num_words\n",
      "  55: tokenizer.fit_on_texts(x_train)\n",
      "  56: \n",
      "  57: vocab_size = len(tokenizer.word_index) + 1  \n",
      "  58: data = []\n",
      "  59: punc = '.'\n",
      "  60: for i in x_train:\n",
      "  61:     i = i[:-1]\n",
      "  62:     token = word_tokenize(i)\n",
      "  63:     if punc in token:\n",
      "  64:         for index ,val in enumerate(token):\n",
      "  65:             if punc == val:\n",
      "  66:                 token.pop(index) \n",
      "  67: #     x_train = tokenizer.texts_to_sequences(token)           \n",
      "  68:     data.append(token)\n",
      "  69: \n",
      "  70: x_train = tokenizer.texts_to_sequences(data) \n",
      "  71: x_test = tokenizer.texts_to_sequences(x_test)\n",
      "  72: \n",
      "  73: maxlen = 100\n",
      "  74: x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
      "  75: x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)\n",
      "  76: #print(x_train)\n",
      "  77: #news_data.info()\n",
      "  78: \n",
      "  79: y_train = np.array(y_train)\n",
      "  80: y_test = np.array(y_test)\n",
      "  81: \n",
      "  82: #print(x_train.shape , vocab_size)\n",
      "  83: #print(type(y_test))\n",
      "  84: \n",
      "  85: y_train = y_train.astype('float64')\n",
      "  86: y_test = y_test.astype('float64')\n",
      "  87: \n",
      "  88: print(('X_train shape:', x_train.shape))\n",
      "  89: print(('X_test shape:', x_test.shape))\n",
      "  90: print(('y_train shape:', y_train.shape))\n",
      "  91: print(('y_test shape:', y_test.shape))\n",
      "  92: \n",
      "  93: \n",
      "  94: \n",
      "  95: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     num_classes = 1 # y_train.shape[1]\n",
      "   4:     num_tokens = 100 #x_train.shape[1]\n",
      "   5:     print(num_classes)\n",
      "   6:     print(num_tokens)\n",
      "   7:     model = Sequential()\n",
      "   8:     model.add(Embedding(vocab_size, output_dim = space['output_dim'], input_length = 100))\n",
      "   9:     model.add(Dropout(space['Dropout']))\n",
      "  10:     model.add(Conv1D(filters = space['filters'], kernel_size = space['kernel_size'], padding='valid', activation='relu', strides=1))\n",
      "  11:     model.add(GlobalMaxPool1D())\n",
      "  12:     model.add(Dense(num_classes))\n",
      "  13:     model.add(Activation('sigmoid'))\n",
      "  14: \n",
      "  15:     model.compile(loss='binary_crossentropy',\n",
      "  16:                   optimizer=space['optimizer'],\n",
      "  17:                   metrics=['acc'])\n",
      "  18: \n",
      "  19:     # Optional to log output from Keras\n",
      "  20:     #csv_logger = keras.callbacks.CSVLogger('Logs/dl_model.log')\n",
      "  21:     \n",
      "  22:     result = model.fit(x_train, y_train,\n",
      "  23:               batch_size=128,\n",
      "  24:               epochs=10,\n",
      "  25:               verbose=0,\n",
      "  26:               validation_split=0.1)\n",
      "  27:     #get the highest validation accuracy of the training epochs\n",
      "  28:     validation_acc = np.amax(result.history['val_acc']) \n",
      "  29:     print('Best validation acc of epoch:', validation_acc)\n",
      "  30:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      "  31: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (7156, 100))\n",
      "('X_test shape:', (240, 100))\n",
      "('y_train shape:', (7156,))\n",
      "('y_test shape:', (240,))\n",
      "1                                                                               \n",
      "100                                                                             \n",
      "Best validation acc of epoch:                                                   \n",
      "1.0                                                                             \n",
      "1                                                                               \n",
      "100                                                                             \n",
      "Best validation acc of epoch:                                                   \n",
      "1.0                                                                             \n",
      "1                                                                               \n",
      "100                                                                             \n",
      "Best validation acc of epoch:                                                   \n",
      "1.0                                                                             \n",
      "1                                                                               \n",
      "100                                                                             \n",
      "Best validation acc of epoch:                                                   \n",
      "1.0                                                                             \n",
      "1                                                                               \n",
      "100                                                                             \n",
      "Best validation acc of epoch:                                                   \n",
      "1.0                                                                             \n",
      "100%|█████████████████████████| 5/5 [02:27<00:00, 29.45s/trial, best loss: -1.0]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "try:\n",
    "    best_run, best_model, space = optim.minimize(model=create_model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=5,\n",
    "                                          trials=Trials(),\n",
    "                                          notebook_name='SepticClassification1',\n",
    "                                          eval_space=True,   # <-- this is the line that puts real values into 'best_run'\n",
    "                                          return_space=True,  # <-- this allows you to save the space for later evaluations\n",
    "                                          )\n",
    "except Exception as e:\n",
    "    logging.exception(\"message\")\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "089b997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import layers\n",
    "# from tensorflow.keras.layers import BatchNormalization\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# def add_conv_block(model):\n",
    "#     model.add(layers.Embedding(vocab_size, 500, input_length=vocab_size))\n",
    "#     model.add(layers.Conv1D(16, 5, activation='relu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(layers.GlobalMaxPooling1D())\n",
    "#     model.add(Dropout(0.5))\n",
    "#     return model\n",
    "# model = Sequential()\n",
    "# model=add_conv_block(model)   \n",
    "# model=add_conv_block(model)\n",
    "\n",
    "# model.add(layers.Dense(10, activation='relu'))\n",
    "# #model.add(layers.Dense(5, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# X=np.array(x_train)\n",
    "# Y=np.array(y_train)\n",
    "# model.fit(X, Y,epochs=10,batch_size=10 , validation_split=0.1 , callbacks= [tensorboard])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from keras import layers\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(layers.Embedding(vocab_size, 16, input_length=100))\n",
    "# model.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "\n",
    "# model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "# model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# X=np.array(x_train)\n",
    "# Y=np.array(y_train)\n",
    "# model.fit(X, Y,epochs=30,batch_size=10 , validation_split=0.1 , callbacks= [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "3cf25d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "2f09432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "# #DataFlair - Initialize a PassiveAggressiveClassifier\n",
    "# pac=PassiveAggressiveClassifier(max_iter=50)\n",
    "# pac.fit(x_train,y_train)\n",
    "\n",
    "# #DataFlair - Predict on the test set and calculate accuracy\n",
    "# y_pred=pac.predict(x_test)\n",
    "# score=accuracy_score(y_test,y_pred)\n",
    "# print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "68c6d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (7156, 100))\n",
      "('X_test shape:', (240, 100))\n",
      "('y_train shape:', (7156,))\n",
      "('y_test shape:', (240,))\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6351 - acc: 0.7583\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = data() \n",
    "acc , y = model.evaluate(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "55bac14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "c91297a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mapper(a):\n",
    "#     a = tokenizer.texts_to_sequences(a)\n",
    "#     return pad_sequences(a, padding='post', maxlen=maxlen)\n",
    "\n",
    "\n",
    "# x_eval = mapper(['he said, \"the world goes round and round and round\"',\n",
    "#         'Even Bangladesh leader Khaleda Zia has said the terrorists are being trained in India and sent to Bangladesh to create disturbance'])\n",
    "\n",
    "# y_eval = np.array([0, 1])\n",
    "# model.evaluate(x_eval, y_eval)\n",
    "\n",
    "# for i in model.predict(x_eval):\n",
    "#     print(round(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "564a230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = [round(i[0]) for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "07b9b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[161  24]\n",
      " [ 34  21]]\n",
      "Accuracy: 75.83%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y = confusion_matrix(y_test, y_pred)\n",
    "print(y)\n",
    "score=accuracy_score(y_test,y_pred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "07f20bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "# score = f1_score(y_test, y_pred, average='binary')\n",
    "# print('F-Measure: %.3f' % score)\n",
    "# from sklearn.metrics import precision_score\n",
    "# precision = precision_score(y_test, y_pred, average='binary')\n",
    "# print('Precision: %.3f' % precision)\n",
    "# from sklearn.metrics import recall_score\n",
    "# recall = recall_score(y_test, y_pred, average='binary')\n",
    "# print('Recall: %.3f' % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "44b7d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from joblib import dump, load\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "88d80003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    #plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "eaeebc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrices(pred,true):\n",
    "    print(confusion_matrix(true,pred))\n",
    "    print(classification_report(true,pred,))\n",
    "    print(\"Accuracy : \",accuracy_score(pred,true))\n",
    "    print(\"Precison : \",precision_score(pred,true, average = 'weighted'))\n",
    "    print(\"Recall : \",recall_score(pred,true,  average = 'weighted'))\n",
    "    print(\"F1 : \",f1_score(pred,true,  average = 'weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "5b5594b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[161  24]\n",
      " [ 34  21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.87      0.85       185\n",
      "         1.0       0.47      0.38      0.42        55\n",
      "\n",
      "    accuracy                           0.76       240\n",
      "   macro avg       0.65      0.63      0.63       240\n",
      "weighted avg       0.74      0.76      0.75       240\n",
      "\n",
      "Accuracy :  0.7583333333333333\n",
      "Precison :  0.7786855036855037\n",
      "Recall :  0.7583333333333333\n",
      "F1 :  0.767236842105263\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_metrices(y_pred,y_test)\n",
    "plot_confusion_matrix(confusion_matrix(y_test,y_pred),target_names=[0,1], normalize = False, \\\n",
    "                      title = 'Confusion matix of CNN on test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "62c35f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_45 (Embedding)     (None, 100, 32)           360064    \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 100, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 86, 50)            24050     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_45 (Glo (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 384,165\n",
      "Trainable params: 384,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "34ff9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "filename='model.h5'\n",
    "tf.keras.models.save_model(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "584e984d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x7f2aa2bb9610>\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.models.load_model('model.h5')\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d884d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03f2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d188e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aaa438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339fdda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
